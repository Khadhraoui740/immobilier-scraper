# Guide d'utilisation - Interface Web d'Administration

## DÃ©marrage

### 1. Installation des dÃ©pendances
```bash
pip install -r requirements.txt
```

### 2. Configuration de l'environnement
Copier `.env.example` en `.env` et remplir avec vos paramÃ¨tres :
```bash
cp .env.example .env
```

### 3. Lancer l'application
```bash
python app.py
```

L'interface est accessible Ã  : **http://localhost:5000**

---

## FonctionnalitÃ©s de l'Interface

### ğŸ“Š Dashboard (Accueil)
- **Vue d'ensemble** : Statistiques principales (total propriÃ©tÃ©s, nouvelles, par source)
- **Actions rapides** :
  - ğŸ”„ **Scraper Maintenant** : Lance le scraping sur tous les sites
  - ğŸ“§ **Tester Email** : Envoie un email de test
  - â° **DÃ©marrer Planificateur** : Lance le scraping automatique

### ğŸ  PropriÃ©tÃ©s
- **Liste des propriÃ©tÃ©s** avec filtres
- **Informations** : Prix, surface, DPE, source, date
- **Ã‰dition** : Mettre Ã  jour le statut de chaque propriÃ©tÃ©
- **Statuts** : disponible, contactÃ©, visitÃ©, rejetÃ©, achetÃ©

### ğŸ” Recherche AvancÃ©e
Filtrer les propriÃ©tÃ©s par :
- ğŸ’° **Prix** : plage min/max
- ğŸ˜ï¸ **Localisation** : zones/codes postaux
- ğŸ“‹ **DPE** : A Ã  G
- â±ï¸ **Statut** : disponible, contactÃ©, etc.

### ğŸ“ Gestion des Sites
- **Voir les scrapers actifs** (SeLoger, PAP, LeBonCoin, BienIci)
- **Activer/DÃ©sactiver** des scrapers
- **Ajouter un nouveau site** :
  - ID unique
  - Nom du site
  - URL de base
  - Timeout (en secondes)
- **Tester** chaque site

### â° Planificateur
Configure le scraping automatique :
- **Intervalle** : Scraper toutes les X heures (dÃ©faut: 2h)
- **Rapport quotidien** : Heure d'envoi du rapport (dÃ©faut: 09:00)
- **Notifications** : Actif/DÃ©sactif
- **Historique** : Voir les exÃ©cutions prÃ©cÃ©dentes

### ğŸ“ˆ Statistiques
- **Distribution par source** : SeLoger, PAP, LeBonCoin, BienIci
- **Distribution par statut** : Disponible, contactÃ©, visitÃ©, etc.
- **Tableaux** avec compteurs et pourcentages

### ğŸ“‹ Logs
- **Visualisation** en temps rÃ©el
- **Auto-refresh** automatique
- **Download** des logs complets
- **Filtrage** par type

---

## Points d'API

L'interface utilise une API REST pour communiquer avec le backend:

### Scraping
```
POST /api/scrape
Body: { "source": "all" | "seloger" | "pap" | "leboncoin" | "bienici" }
```

### Recherche
```
POST /api/search
Body: {
  "price_min": 200000,
  "price_max": 500000,
  "dpe_max": "D",
  "location": "75",
  "status": "disponible"
}
```

### PropriÃ©tÃ©s
```
GET /api/property/{id}
POST /api/property/{id}
Body: { "status": "nouveau_statut" }
```

### Sites
```
GET /api/sites                      # Liste les sites
PUT /api/sites/{id}                 # Activer/DÃ©sactiver
Body: { "enabled": true/false }
POST /api/sites/new                 # Ajouter un site
Body: { "id": "...", "name": "...", "url": "...", "timeout": 30 }
```

### Planificateur
```
POST /api/scheduler/start           # DÃ©marrer
POST /api/scheduler/stop            # ArrÃªter
GET /api/scheduler/status           # Ã‰tat actuel
```

### Statistiques
```
GET /api/stats                      # Toutes les stats
```

---

## Configuration avancÃ©e

### Filtres de recherche
Modifiez `config.py` pour changer les critÃ¨res par dÃ©faut:

```python
SEARCH_CONFIG = {
    'budget_min': 200000,
    'budget_max': 500000,
    'dpe_max': 'D',
    'zones': ['Paris', 'Hauts-de-Seine', 'Val-de-Marne']
}
```

### Scheduler
```python
SCHEDULER_CONFIG = {
    'interval_hours': 2,        # Intervalle en heures
    'max_workers': 3,           # Scrapers parallÃ¨les
    'retry_failed_after_minutes': 30
}
```

### Email
```python
EMAIL_CONFIG = {
    'email': 'khadhraoui.jalel@gmail.com',
    'smtp_server': 'smtp.gmail.com',
    'smtp_port': 587
}
```

---

## Ajouter un nouveau scraper

### 1. CrÃ©er le scraper (e.g., `scrapers/newsite_scraper.py`)
```python
from .base_scraper import BaseScraper

class NewSiteScraper(BaseScraper):
    def __init__(self, config=None):
        super().__init__("NewSite", config)
    
    def scrape(self, filters):
        # Votre logique de scraping
        pass
```

### 2. Ajouter Ã  `config.py`
```python
SCRAPERS_CONFIG = {
    'newsite': {
        'name': 'NewSite',
        'url': 'https://...',
        'enabled': True,
        'timeout': 30
    }
}
```

### 3. Importer dans `scrapers/manager.py`
```python
from .newsite_scraper import NewSiteScraper

# Dans _init_scrapers():
if SCRAPERS_CONFIG['newsite']['enabled']:
    self.scrapers['newsite'] = NewSiteScraper(SCRAPERS_CONFIG['newsite'])
```

### 4. Utiliser l'interface pour l'activer
L'interface web dÃ©tectera automatiquement le nouveau scraper!

---

## DÃ©pannage

### "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### "Port 5000 dÃ©jÃ  utilisÃ©"
Modifier dans `app.py`:
```python
app.run(host='0.0.0.0', port=5001, debug=True)
```

### Emails ne s'envoient pas
1. VÃ©rifier les identifiants dans `.env`
2. Pour Gmail : utiliser un [mot de passe d'application](https://myaccount.google.com/apppasswords)
3. VÃ©rifier les logs

### Scrapers trop lent
Augmenter le timeout dans la configuration des sites

---

## Architecture

```
immobilier-scraper/
â”œâ”€â”€ app.py                  # Application Flask
â”œâ”€â”€ config.py               # Configuration centralisÃ©e
â”œâ”€â”€ database/db.py          # Gestion SQLite
â”œâ”€â”€ scrapers/               # Modules de scraping
â”‚   â”œâ”€â”€ manager.py          # Gestionnaire parallÃ¨le
â”‚   â”œâ”€â”€ bienici_scraper.py  # âœ¨ Nouveau
â”‚   â””â”€â”€ ...
â”œâ”€â”€ templates/              # Templates HTML/Jinja2
â”œâ”€â”€ static/                 # CSS et JavaScript
â””â”€â”€ requirements.txt        # DÃ©pendances
```

---

## Support

Email: khadhraoui.jalel@gmail.com
